
---
title: 梯度下降笔记
date: 2017-09-15 22:54:44
tags:
---
        # Gradient Descent

## Tuning your learning rates

-------

首先用可视化的方法，制作损失值关于迭代次数的折线图，要保证，损失值随着迭代次数逐渐下降，而不是上升，或者快速平稳。（如果参数过多，制作损失值关于参数的变化可视化是很困难的,所以一般不制作那样的图）

------
### Adaptive Learning Rates

因为，在一开始的时候，初始参数离最优参数一般是很远的，随着迭代的进行，参数离最优参数的距离变近。所以，一个很朴素的想法就是，然学习率随着迭代次数变小。一种自适应学习率的方法如下：

```math
\eta^t = \eta/\sqrt{t+1} 

```
`$\eta^t$`代表第t次迭代的学习率。

#### Adagrad

对于每个参数应用相同的学习率是不合适的。Adagrad对不同的参数应用不同的学习率。

Adagrad的做法是，每次更新学习率的时候，除了应用前面的衰减策略以外，还会除以该参数先前偏导数的均方根。

```math

\omega^{t+1} = \omega^t - \frac{\eta^t}{\sigma^t}g^t

g^t = \frac{\partial L}{\partial \omega}

\sigma^t = \sqrt{\frac{\sum_t (g^t)^2}{t+1}}

\eta^t = \eta/\sqrt{t+1} 

\omega^{t+1} = \omega^t - \frac{\eta}{\sqrt{\sum_t(g^t)^2}}g^t
```

（Adagrad的直观理解是，它会夸张一个梯度，如果一个梯度和先前的梯度相比是比较大的，那么它会被放大很多，如果一个梯度和先前的梯度相比是比较小的，那么它会被缩小很多。）

可以直观的证明，最优的步长，应该和
```math
\frac{First Derivation}{Second Derivation}
```
成正比。（可以考虑`$y=ax^2 + bx+c$`）的情况。

所以，更新的步长应该是，`$-\eta\frac{First Derivation}{Second Derivation}$`

观察这个式子和Adagrad的区别，可以发现Adagrad使用
`$\sqrt{\sum_o^t(g^i)^2}$`来近似Second Derivation


## Stochastic Gradient Descent

随机梯度下降和一般的梯度下降的区别在于。

随机梯度下降，计算一个样本点的损失函数
```math
l = l(y, f(x))
```
然后来做梯度下降。


而一般的梯度下降，计算所有样本的损失函数
```math
L = \sum_n l(y^n, f(x^n))

```
然后做梯度下降。


使用随机梯度下降的好处，在于它的速度非常的快。


## Feature Scaling

如果样本的不同特征它们的scale相差很大，那么对于scale大的那个特征那说，和它对应的那个权重，即使发生了很小的变化，整个输出也会发生很大的变化。

这导致了，损失函数在这个方向的变化很快，而在另一scale小的方向上变化很小。


### 方法

一个做Featire Scaling 的方法就是，和标准化类似，
计算特征的均值和标准差`$\mu, \sigma$`,然后对特征值做如下操作：
```math
x_i^r = \frac{x_i^r - \mu}{\sigma}
```
这个操作，使得这个特征的均值变为0，方差变为1

## Gradient Descent Theory

假如定义我们的损失函数为
```math
L(\theta_1, \theta_2)
```
那么我们的优化问题可以描述为：
```math
(\theta_1^*, \theta_2^*)=arg_{\theta_1, \theta_2}minL(\theta_1, \theta_2)

```

想象一下， 我们在以`$\theta_1, \theta_2$`为坐标轴的平面上，从随机选定的一个点a开始。

如果在a的一个邻域上，我们可以找到一个点b，使得L(b)的值，是邻域内的最小值。

我们在b的邻域内重复这个操作，知道这个点稳定为止，我们就可以找到一个局部极小值。


现在如果我们能够在一个邻域内找到L的极小值，那么就可以找到局部极小值。


为了在一个以(a,b)为圆心的小邻域内找到L的极小值，我们可以用泰勒展开近似L
```math
L(\theta_1, \theta_2) = 
L(a, b) + \frac{\partial L(a, b)}{\partial \theta_1}
(\theta_1 - a)
+
\frac{\partial L(a,b)}{\partial \theta_2}(\theta_2 - b)


Let: s = L(a,b)

Let: u = \frac{\partial L(a, b)}{\partial \theta_1}

Let: v = \frac{\partial L(a,b)}{\partial \theta_2}

so: L = s + u(\theta_1 - a) + v(\theta_2 - b)


```

为了求L的最小值。只要求
```math

u(\theta_1 - a) + v(\theta_2 - b)

```
的最小值。

因为
```math

u(\theta_1 - a) + v(\theta_2 - b)
=(u, v) dot (\theta_1 - a, \theta_2 - b)^T

```

只要令：
`$(\theta_1 - a, \theta_2 - b)$`的方向和（u,v）相反，长度尽可能大即可。

```math
(\theta_1 - a, \theta_2 - b) = -\eta(u, v)

\theta = (a,b)^T - \nabla L(a, b)

```






























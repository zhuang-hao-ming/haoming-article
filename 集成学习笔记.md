
---
title: 集成学习笔记
date: 2017-09-15 22:54:44
tags:
---
        ## Bagging

分类器存在Bias和Variance之间的trade-off。一个模型越复杂，那么这个模型的Bias就越小，受数据的影响就大，Variance就越大。

由于Bias和Variance之间的trade-off，一个模型的整体误差随着模型复杂度的增大，呈现一个先减少后增加的趋势。

复杂度很低的模型，往往有underfitting的现象，而复杂度很高的模型则往往有overfitting的现象。


根据统计学上的知识，假设`$f1, f2,f3, ..., fn$`有相同的方差`$d$`，根据方差的性质则`$\frac{f1+f2+f3+ ...+fn}{n}$`的方差是`$\frac{d}{n}$`

基于上面的介绍，我们用一般的话语描述Bagging的策略如下：

对于一个容量为`$N$`的训练集我们用有放回的采样方法，采样`$N'$`个样本，一般`$N = N'$`。得到多个新的训练集，然后用每个训练集分别训练*预先设计好的基础分类器*,得到多个基础分类器。

对于输入的测试数据`$x$`，我们将它分别输入基础分类器中，分别得到结果，然后根据任务的特点，对这些结果进行平均或者投票。

**注意： 根据我们前面的描述，Bagging这样的策略适合那些，本身比较复杂的，容易过拟合的分类器，Bagging能够改善这些基分类器的过拟合现象**


## 随机森林

因为决策树是一个强分类器，它常常可以作为Bagging的基础分类器。随机森林就是决策树的Bagging。

Bagging要求基础分类器之间应该尽量的不同和互补。

为了增加随机性，随机森林和一般的Bagging策略相比。
它还限制了，分割节点的特征应该是随机的。

### OOB(Out of Bag) error

## Boosting

和Bagging的策略不同，Bagging的目的是降低一个强分类器的Variance，来提升分类器的性能。

Boosting的目的是，降低一个弱分类器的Bias来提升分类器的性能。


Boostring的想法是，

我们首先得到一个弱分类器`$f_1(x)$`，然后另一个分类器`$f_2(x)$`来辅助`$f_1(x)$`，提升整体性能。

可以直观的知道，如果`$f_2(x)$`和`$f_1(x)$`很类似，那么整体性能是没有提升的。`$f_2(x)$`和`$f_1(x)$`最好能够互补，那样整体性能就可以提升。


### 如何得到不同的分类器？

一个直观的想法就是，使用不同的数据集来训练分类器，那么将会得到不同的分类器。

从前面的Bagging方法中，我们已经知道了，可以使用有放回的随机采样的方法来得到不同的训练数据集。

但是使用这样一个方法，我们只能使一个样本在一个训练数据集中被使用整数次。

如果我们可以使，一个样本在一个训练数据集中使用的次数是浮点数的，效果应该更好。

所以我们可以给每个样本点增加一个权重`$u^n$`

这样一个操作，相当于，我们把我们的损失函数修改成了：

```math
L(f) = \sum u^nl(f(x^n), y^n)
```

### Adaboost

根据前面的介绍，Adaboost为了实现前面说的，让`$f_2(x) ,f_1(x)$`互补，它需要找一个不同的训练数据集来训练`$f_2(x)$`。

什么样的训练数据集才足够不同呢？

Adaboost的思路是，它要找一个训练数据集，这个训练数据集使得`$f_1(x)$`的性能变为一个随机分类器。

我们用公式化的方法来描述一下：

定义`$f_1(x)$`在训练数据集上的错误率ε1
```math

\epsilon_1 = 
\frac{\sum_{N} u_1^n\delta(f_1(x^n) \neq y^n)}{Z_1}

Z_1 = \sum_Nu_1^n

\epsilon_1 < 0.5

```

δ是一个示性函数，当`$f(x^n) \neq y^n$`时返回1，否则返回0.

**ε1<0.5说明，我们的基础分类器，至少要比随机分类器好，当然我们无法得到一个ε1>0.5的分类器，因为如果有只要简单的将分类器取反，即可以得到ε1<0.5的分类器**


找一个新的训练数据集，意味着，找一个新的权重`$u_2^n$`。

使得`$f_1(x)$`的性能变成一个随机分类器，意味着：
```math

\frac{\sum_{N} u_2^n\delta(f_1(x^n) \neq y^n)}{Z_2}
=0.5

Z2 = \sum_N u_2^n

```


如何修改权重来使上面的式子成立：

我们规定：

```math
If: f(x^n) \neq y^n 

u_2^n = u_1^n * d1

Else: f(x^n) = y^n

u_2^n = u_1^n / d1

```
根据ε的定义，我们很容易知道，为了使f1的性能降低，我们要增加错误的样本点的权重，降低正确的样本点的权重。

### 计算d

```math
Let:  \frac{\sum_n u_2^n \delta(f_1(x^n) \neq y^n)}{Z_2}=0.5

\sum_n u_2^n \delta(f_1(x^n) \neq y^n) = \sum_{f_1(x^n) \neq y^n}u_1^nd1

Z_2 = \sum_n u_2^n
= \sum_{f_1(x^n) \neq y^n}u_1^nd1 + 
\sum_{f_1(x^n) = y^n}u_1^n/d1

\frac{\sum_{f_1(x^n) \neq y^n}u_1^nd1}{\sum_{f_1(x^n) \neq y^n}u_1^nd1 + 
\sum_{f_1(x^n) = y^n}u_1^n/d1}
= 0.5

\frac{\sum_{f_1(x^n) \neq y^n}u_1^nd1 + 
\sum_{f_1(x^n) = y^n}u_1^n/d1}{\sum_{f_1(x^n) \neq y^n}u_1^nd1}
=2


1 + \frac{\sum_{f_1(x^n) = y^n}u_1^n/d1}{\sum_{f_1(x^n) \neq y^n}u_1^nd1}
= 2

\frac{\sum_{f_1(x^n) = y^n}u_1^n/d1}{\sum_{f_1(x^n) \neq y^n}u_1^nd1}
=1

\sum_{f_1(x^n) = y^n}u_1^n/d1
=
\sum_{f_1(x^n) \neq y^n}u_1^nd1

1/d1\sum_{f_1(x^n) = y^n}u_1^n
=
d1\sum_{f_1(x^n) \neq y^n}u_1^n

beacuse:
\epsilon_1 = \frac{ \sum_{f_1(x^n) \neq y^n}u_1^n }{Z_1}


1-\epsilon = \frac{\sum_{f_1(x^n) = y^n}u_1^n}{Z_1}


1/d1*(1-\epsilon_1)*Z_1=
d1*Z_1*\epsilon_1

d1 = \sqrt{(1-\epsilon_1) / \epsilon_1}

d1 > 1


```

### Adaboost算法描述

1. 对于一个给定的训练数据集。
2. 在迭代1，...,t,.., T中
3. 用权重`$u_t$`训练一个弱分类器，`$f_t(x)$`
4. `$\epsilon_t$`是`$f_t(x)$`的错误率
5. 修改权重：
如果，`$x^n \neq f(x^n)$` 那么 `$u_{t+1}^n = u_t^n * d_t$`
如果，`$x^n = f(x^n)$` 那么 `$u_{t+1}^n = u_t^n / d_t$`

```math

d_t = \sqrt{(1-\epsilon_t)/\epsilon_t}

Let: \alpha_t = ln(d_t)

u_{t+1}^n = u_t^n * exp(\alpha_t) (when:y^n \neq f_t(x^n))

u_{t+1}^n = u_t^n / exp(\alpha_t) = u_t^n * exp(-\alpha_t) (when: y^n = f_t(x^n))



u_{t+1}^n = u_t^n * exp(-y^nf_t(x)\alpha_t)


```




### 如何aggregate

```math
H(x) = \sum_t \alpha_tf_t(x)
```

因为`$\alpha_t = ln(\sqrt{(1-\epsilon_t)/\epsilon_t})$`

如果一个分类器的误差越小，那么这个分类器对应的α就会更加大。

所以把α作为权重是合理。


### 证明Adaboost的错误率的上界收敛到0

定义：
`$ H(X) = sign(\sum_t \alpha_tf_t(x)) $`

则它的错误率定义为：
`$\epsilon = \frac{\sum \delta(H(X^n) \neq y^n)}{N}$`


令:`$\sum_t \alpha_tf_t(x) = g(x)$`

错误率也可以写作：
```math

\epsilon = \frac{\sum \delta(g(x^n)y^n) < 0}{N}

\epsilon <  \frac{\sum exp(-y^ng(x^n))}{N}

```
上面这个式子，可以认为把理想的损失函数，放大到它的一个上界指数函数。


```math
u_1^n = 1

u_{t+1}^n = u_t^n * exp(-y^nf_t(x^n)\alpha_t) 

u_{T+1}^n = \prod_texp(-y^nf_t(x^n)\alpha_t) 

Z_{T+1} = \sum_n u_{T+1}^n

= \sum_n\prod_texp(-y^nf_t(x^n)\alpha_t)

= \sum_nexp(\sum_t -y^nf_t(x^n)\alpha_t)

= \sum_nexp(-y^n\sum_t f_t(x^n)\alpha_t)

= \sum_nexp(-y^ng(x))

```

根据上面的式子可以得到：
```math
\epsilon <  \frac{exp(-y^ng(x^n))}{N}
=Z_{T+1}/N

```

因为：
```math
Z_1 = N

Z_{t+1} = Z_t * \epsilon_t * exp(\alpha_t) + 
Z_t * (1-\epsilon_t) * exp(-\alpha_t)

= Z_t * \epsilon_t * \sqrt{(1-\epsilon_t)/\epsilon_t}

+ Z_t * (1 - \epsilon_t) * \sqrt{\epsilon_t/(1-\epsilon_t)}

= Z_t * \sqrt{(1-\epsilon_t)\epsilon_t}

+ Z_t * \sqrt{(1-\epsilon_t)\epsilon_t}

= Z_t * 2 * \sqrt{(1-\epsilon_t)\epsilon_t}


Z_{T+1} = N\prod_t2 * \sqrt{(1-\epsilon_t)\epsilon_t}

because :

2 * \sqrt{(1-\epsilon_t)\epsilon_t} < 1


\epsilon <  \frac{exp(-y^ng(x^n))}{N}
=Z_{T+1}/N

= \prod_t2 * \sqrt{(1-\epsilon_t)\epsilon_t} < 1

```
收敛到0.


### Adaboost特点

Adaboost有一个特点是，即使在迭代一定次数以后，训练的误差得到了0，但是测试误差，仍然会随着迭代次数下降。


这一特点出现的原因，可以认为和SVM一样理解。

因为随着进一步迭代，前面的误差上界，将会越来越小，相当于margin在增大。


## General Formulation Boosting

Adaboost是提升的一个特例。

可以使用梯度下降来在给定一个损失函数的情况下，来训练一个任意的boosting。


## stacking

把多个分类器的输出，输入到一个新的分类器中，训练它们的组合权重。

这个和简单的投票和平均，或者手工的给定一个权重，是一个更好的集成方式。






















